From 7143300322499465d5956222440b612a3b728ab3 Mon Sep 17 00:00:00 2001
From: Zhenyu Wang <zhenyuw@linux.intel.com>
Date: Thu, 4 May 2017 15:02:40 +0800
Subject: [PATCH 1/2] POC: Add GPU cgroup subsys

This adds new GPU cgroup subsys which is used for GPU resource
control against processes in cgroup hierarchy. Currently added
memory and priority setting for GPU cgroup, drm gem object is
accounted for memory consumption and drm/i915 context priority
is taken from processes' cgroup setting.

Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
---
 drivers/gpu/drm/drm_file.c                 |   1 +
 drivers/gpu/drm/drm_gem.c                  |  43 ++++++++-
 drivers/gpu/drm/i915/i915_drv.h            |   2 +
 drivers/gpu/drm/i915/i915_gem.c            |   4 +
 drivers/gpu/drm/i915/i915_gem_context.c    |   2 +
 drivers/gpu/drm/i915/i915_gem_execbuffer.c |   3 +
 include/drm/drm_file.h                     |   7 ++
 include/drm/drm_gem.h                      |   5 ++
 include/linux/cgroup_gpu.h                 |  15 ++++
 include/linux/cgroup_subsys.h              |   4 +
 init/Kconfig                               |   7 ++
 kernel/cgroup/Makefile                     |   1 +
 kernel/cgroup/gpu.c                        | 140 +++++++++++++++++++++++++++++
 13 files changed, 233 insertions(+), 1 deletion(-)
 create mode 100644 include/linux/cgroup_gpu.h
 create mode 100644 kernel/cgroup/gpu.c

diff --git a/drivers/gpu/drm/drm_file.c b/drivers/gpu/drm/drm_file.c
index 3783b65..73dac9b 100644
--- a/drivers/gpu/drm/drm_file.c
+++ b/drivers/gpu/drm/drm_file.c
@@ -222,6 +222,7 @@ static int drm_open_helper(struct file *filp, struct drm_minor *minor)
 	INIT_LIST_HEAD(&priv->pending_event_list);
 	INIT_LIST_HEAD(&priv->event_list);
 	init_waitqueue_head(&priv->event_wait);
+	spin_lock_init(&priv->obj_stat_lock);
 	priv->event_space = 4096; /* set aside 4k for event buffer */
 
 	mutex_init(&priv->event_read_lock);
diff --git a/drivers/gpu/drm/drm_gem.c b/drivers/gpu/drm/drm_gem.c
index b1e28c9..11a207e 100644
--- a/drivers/gpu/drm/drm_gem.c
+++ b/drivers/gpu/drm/drm_gem.c
@@ -775,8 +775,12 @@ drm_gem_object_release(struct drm_gem_object *obj)
 {
 	WARN_ON(obj->dma_buf);
 
-	if (obj->filp)
+	if (obj->filp) {
+		struct drm_file *file_priv = obj->filp->private_data;
+		if (file_priv)
+			drm_gem_obj_del_mem(file_priv, obj->size);
 		fput(obj->filp);
+	}
 
 	drm_gem_free_mmap_offset(obj);
 }
@@ -1004,3 +1008,40 @@ int drm_gem_mmap(struct file *filp, struct vm_area_struct *vma)
 	return ret;
 }
 EXPORT_SYMBOL(drm_gem_mmap);
+
+int drm_gem_obj_check_max_mem(struct drm_file *file_priv, u64 size)
+{
+	int ret = 0;
+	struct task_struct *task;
+	
+	spin_lock(&file_priv->obj_stat_lock);
+
+	task = get_pid_task(file_priv->pid, PIDTYPE_PID); //获取task的pid
+	if (task) {
+		if (file_priv->obj_mem + size > gpucg_get_max_mem(task)) {  //检查是否达到cgroup分配的最大memory
+			ret = 1;
+			DRM_DEBUG_DRIVER("ZHEN: hit max mem %d\n", task->pid);
+		}
+		put_task_struct(task);
+	}
+	spin_unlock(&file_priv->obj_stat_lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(drm_gem_obj_check_max_mem);
+
+void drm_gem_obj_add_mem(struct drm_file *file_priv, u64 size)
+{
+	spin_lock(&file_priv->obj_stat_lock);
+	file_priv->obj_mem += size;                //扩大对应对象的memory
+	spin_unlock(&file_priv->obj_stat_lock);
+}
+EXPORT_SYMBOL(drm_gem_obj_add_mem);
+
+void drm_gem_obj_del_mem(struct drm_file *file_priv, u64 size)
+{
+	spin_lock(&file_priv->obj_stat_lock);
+	file_priv->obj_mem -= size;               //缩减对应对象的memory
+	spin_unlock(&file_priv->obj_stat_lock);
+}
+EXPORT_SYMBOL(drm_gem_obj_del_mem);
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index 357b6c6..d9adc93 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -51,6 +51,8 @@
 #include <drm/drm_auth.h>
 #include <drm/drm_cache.h>
 
+#include <linux/cgroup_gpu.h>
+
 #include "i915_params.h"
 #include "i915_reg.h"
 #include "i915_utils.h"
diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c
index 33fb11c..6566ff5 100644
--- a/drivers/gpu/drm/i915/i915_gem.c
+++ b/drivers/gpu/drm/i915/i915_gem.c
@@ -657,6 +657,9 @@ i915_gem_create(struct drm_file *file,
 	if (size == 0)
 		return -EINVAL;
 
+	if (drm_gem_obj_check_max_mem(file, size))
+		return -ENOMEM;
+	
 	/* Allocate the new object */
 	obj = i915_gem_object_create(dev_priv, size);
 	if (IS_ERR(obj))
@@ -668,6 +671,7 @@ i915_gem_create(struct drm_file *file,
 	if (ret)
 		return ret;
 
+	drm_gem_obj_add_mem(file, size);
 	*handle_p = handle;
 	return 0;
 }
diff --git a/drivers/gpu/drm/i915/i915_gem_context.c b/drivers/gpu/drm/i915/i915_gem_context.c
index 8bd0c49..7c7109b 100644
--- a/drivers/gpu/drm/i915/i915_gem_context.c
+++ b/drivers/gpu/drm/i915/i915_gem_context.c
@@ -377,6 +377,8 @@ i915_gem_create_context(struct drm_i915_private *dev_priv,
 		ctx->desc_template = default_desc_template(dev_priv, ppgtt);
 	}
 
+	ctx->priority = gpucg_get_priority(current);    //获取当前任务的优先级
+
 	trace_i915_context_create(ctx);
 
 	return ctx;
diff --git a/drivers/gpu/drm/i915/i915_gem_execbuffer.c b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
index af19657..a389271 100644
--- a/drivers/gpu/drm/i915/i915_gem_execbuffer.c
+++ b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
@@ -1648,6 +1648,9 @@ i915_gem_do_execbuffer(struct drm_device *dev, void *data,
 
 	i915_gem_context_get(ctx);
 
+	/* a little hack for cg prio setting anyway */
+	ctx->priority = (int)gpucg_get_priority(current);
+	
 	if (ctx->ppgtt)
 		vm = &ctx->ppgtt->base;
 	else
diff --git a/include/drm/drm_file.h b/include/drm/drm_file.h
index 5dd27ae..3575eed 100644
--- a/include/drm/drm_file.h
+++ b/include/drm/drm_file.h
@@ -32,6 +32,7 @@
 
 #include <linux/types.h>
 #include <linux/completion.h>
+#include <linux/cgroup_gpu.h>
 
 #include <uapi/drm/drm.h>
 
@@ -311,6 +312,12 @@ struct drm_file {
 
 	/* private: */
 	unsigned long lock_count; /* DRI1 legacy lock count */
+
+	/**
+	 * per-client mem accounting
+	 */
+	spinlock_t obj_stat_lock;
+	u64 obj_mem;
 };
 
 /**
diff --git a/include/drm/drm_gem.h b/include/drm/drm_gem.h
index 663d803..10ba8b8 100644
--- a/include/drm/drm_gem.h
+++ b/include/drm/drm_gem.h
@@ -321,4 +321,9 @@ int drm_gem_dumb_destroy(struct drm_file *file,
 			 struct drm_device *dev,
 			 uint32_t handle);
 
+int drm_gem_obj_check_max_mem(struct drm_file *file_priv, u64 size);
+void drm_gem_obj_add_mem(struct drm_file *file_priv, u64 size);
+void drm_gem_obj_del_mem(struct drm_file *file_priv, u64 size);
+
+
 #endif /* __DRM_GEM_H__ */
diff --git a/include/linux/cgroup_gpu.h b/include/linux/cgroup_gpu.h
new file mode 100644
index 0000000..eb2c2ae
--- /dev/null
+++ b/include/linux/cgroup_gpu.h
@@ -0,0 +1,15 @@
+/*
+ * Copyright (C) 2017 Zhenyu Wang <zhenyuw@linux.intel.com>
+ *
+ * This file is subject to the terms and conditions of version 2 of the GNU
+ * General Public License. See the file COPYING in the main directory of the
+ * Linux distribution for more details.
+ */
+
+#ifndef CGROUP_GPU_H
+#define CGROUP_GPU_H
+
+extern s64 gpucg_get_priority(struct task_struct *task);   //获取任务的优先级
+extern u64 gpucg_get_max_mem(struct task_struct *task);    //获取任务内存上限
+
+#endif
diff --git a/include/linux/cgroup_subsys.h b/include/linux/cgroup_subsys.h
index d0e597c..6d277ee 100644
--- a/include/linux/cgroup_subsys.h
+++ b/include/linux/cgroup_subsys.h
@@ -67,6 +67,10 @@ SUBSYS(rdma)
 SUBSYS(debug)
 #endif
 
+#if IS_ENABLED(CONFIG_CGROUP_GPU)
+SUBSYS(gpu)
+#endif
+
 /*
  * DO NOT ADD ANY SUBSYSTEM WITHOUT EXPLICIT ACKS FROM CGROUP MAINTAINERS.
  */
diff --git a/init/Kconfig b/init/Kconfig
index a92f27d..354bce8 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1174,6 +1174,13 @@ config CGROUP_DEBUG
 
 	  Say N.
 
+config CGROUP_GPU
+	bool "GPU controller"
+	depends on DRM
+	default n
+	help
+	  Provides GPU cgroup controller.
+
 config SOCK_CGROUP_DATA
 	bool
 	default n
diff --git a/kernel/cgroup/Makefile b/kernel/cgroup/Makefile
index 387348a..9d53bfb 100644
--- a/kernel/cgroup/Makefile
+++ b/kernel/cgroup/Makefile
@@ -4,3 +4,4 @@ obj-$(CONFIG_CGROUP_FREEZER) += freezer.o
 obj-$(CONFIG_CGROUP_PIDS) += pids.o
 obj-$(CONFIG_CGROUP_RDMA) += rdma.o
 obj-$(CONFIG_CPUSETS) += cpuset.o
+obj-$(CONFIG_CGROUP_GPU) += gpu.o
diff --git a/kernel/cgroup/gpu.c b/kernel/cgroup/gpu.c
new file mode 100644
index 0000000..12e77d9
--- /dev/null
+++ b/kernel/cgroup/gpu.c
@@ -0,0 +1,140 @@
+/*
+ * GPU cgroup
+ *
+ * GPU resource control based on cgroup hierarchy
+ *
+ * Copyright (C) 2017 Zhenyu Wang <zhenyuw@linux.intel.com>
+ *
+ * This file is subject to the terms and conditions of version 2 of the GNU
+ * General Public License. See the file COPYING in the main directory of the
+ * Linux distribution for more details.
+ */
+
+#include <linux/slab.h>
+#include <linux/cgroup.h>
+#include <linux/cgroup_gpu.h>
+
+struct gpu_cgroup {
+	struct cgroup_subsys_state css;
+	s64 prio;
+	u64 max_mem_in_bytes;
+};
+
+static DEFINE_MUTEX(gpucg_mutex);
+
+#define GPU_PRIO_MAX 1024 /* align with i915 now */        //定义任务优先级范围
+
+static struct gpu_cgroup *css_gpucg(struct cgroup_subsys_state *css)
+{
+	return container_of(css, struct gpu_cgroup, css);
+}
+
+static inline struct gpu_cgroup *get_current_gpucg(void)   //获取当前gpu的cgroup
+{
+	return css_gpucg(task_get_css(current, gpu_cgrp_id));
+}
+
+static inline struct gpu_cgroup *get_task_gpucg(struct task_struct *task)  //
+{
+	return css_gpucg(task_get_css(task, gpu_cgrp_id));
+}
+
+static s64 gpu_css_get_prio(struct cgroup_subsys_state *css, struct cftype *cft) //获取优先级
+{
+	struct gpu_cgroup *cg = css_gpucg(css);
+
+	return cg->prio;
+}
+
+static int gpu_css_set_prio(struct cgroup_subsys_state *css, struct cftype *cft, //设置优先级
+			    s64 val)
+{
+	struct gpu_cgroup *cg = css_gpucg(css);
+
+	if (val > GPU_PRIO_MAX || val < -GPU_PRIO_MAX)  //超出优先级限制
+		return -EINVAL;
+	
+	mutex_lock(&gpucg_mutex);
+	cg->prio = val;       			//赋值优先级
+	mutex_unlock(&gpucg_mutex);
+	return 0;
+}
+
+static u64 gpu_css_get_max_mem(struct cgroup_subsys_state *css, struct cftype *cft)  //获取最大memory
+{
+	struct gpu_cgroup *cg = css_gpucg(css);
+
+	return cg->max_mem_in_bytes;
+}
+
+static int gpu_css_set_max_mem(struct cgroup_subsys_state *css, struct cftype *cft,  //设置最大memory
+			       u64 val)
+{
+	struct gpu_cgroup *cg = css_gpucg(css);
+
+	mutex_lock(&gpucg_mutex);
+	cg->max_mem_in_bytes = val;
+	mutex_unlock(&gpucg_mutex);
+	return 0;
+}
+
+static struct cftype gpu_css_files[] = {
+	{
+		.name = "priority",
+		.write_s64 = gpu_css_set_prio,
+		.read_s64 = gpu_css_get_prio,
+		.flags = CFTYPE_NOT_ON_ROOT,
+	},
+	{
+		.name = "max_mem_in_bytes",
+		.write_u64 = gpu_css_set_max_mem,
+		.read_u64 = gpu_css_get_max_mem,
+		.flags = CFTYPE_NOT_ON_ROOT,
+	},
+};
+
+s64 gpucg_get_priority(struct task_struct *task)
+{
+	struct gpu_cgroup *cg = get_task_gpucg(task);
+
+	BUG_ON(!cg);
+	return cg->prio;
+}
+EXPORT_SYMBOL(gpucg_get_priority);
+
+u64 gpucg_get_max_mem(struct task_struct *task)
+{
+	struct gpu_cgroup *cg = get_task_gpucg(task);
+	BUG_ON(!cg);
+	return cg->max_mem_in_bytes;
+}
+EXPORT_SYMBOL(gpucg_get_max_mem);
+
+static struct cgroup_subsys_state *
+gpu_css_alloc(struct cgroup_subsys_state *parent_css)
+{
+	struct gpu_cgroup *cg;
+	
+	cg = kzalloc(sizeof(*cg), GFP_KERNEL);
+	if (!cg)
+		return ERR_PTR(-ENOMEM);
+
+	cg->prio = 0;
+	cg->max_mem_in_bytes = ULONG_MAX;
+	return &cg->css;
+}
+
+static void gpu_css_free(struct cgroup_subsys_state *css)
+{
+	struct gpu_cgroup *cg = css_gpucg(css);
+
+	kfree(cg);
+}
+
+
+struct cgroup_subsys gpu_cgrp_subsys = {
+	.css_alloc = gpu_css_alloc,
+	.css_free = gpu_css_free,
+	.legacy_cftypes = gpu_css_files,
+	.dfl_cftypes = gpu_css_files,
+};
-- 
2.7.4

